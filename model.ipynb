{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.utils\n",
    "import random\n",
    "\n",
    "# example topics\n",
    "topics = [\"change_artwork\", \"change_product\", \"change_size\", \"change_file\", \"blank\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose W2V pretrained model.\n",
    "\n",
    "*Loading this might take some time, also beware that loading big dataset requires 16-20 GB of RAM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# word_vectors = KeyedVectors.load_word2vec_format(\"../w2v/glove.6B.50d.txt\", binary=False)\n",
    "# EMBEDDING_DIM=50\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../w2v/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "EMBEDDING_DIM=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data and split it into sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/my_data.csv')\n",
    "set_blanks = lambda x: x if isinstance(x, str) else \"blank\"\n",
    "data['topic'] = data['topic'].apply(set_blanks)\n",
    "data.text=data.text.astype(str)\n",
    "data.topic=data.topic.astype(str)\n",
    "data = data[data.topic.notnull()]\n",
    "data = sklearn.utils.shuffle(data)\n",
    "\n",
    "data_train, data_valid_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "data_valid, data_test = train_test_split(data_valid_test, test_size=0.5, random_state=42)\n",
    "del(data_valid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data prepropcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find topic that occurs most frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "for topic in topics:\n",
    "    topic_dict[topic] = []\n",
    "\n",
    "max_count = 0\n",
    "for index, row in data_train.iterrows():\n",
    "    topic_dict[row['topic']].append(row['text'])\n",
    "    new_length = len(topic_dict[row['topic']])\n",
    "    if new_length > max_count:\n",
    "        max_count = new_length\n",
    "max_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences for other topics so number of examples is of the same length.\n",
    "\n",
    "When generative replace half of words with random synonyms. \n",
    "\n",
    "**THIS IS SLOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic in topics:\n",
    "    print(topic)\n",
    "    for i in range(max_count - len(topic_dict[topic])):\n",
    "        print(i)\n",
    "        random_text = random.choice(topic_dict[topic])\n",
    "        words = random_text.split(\" \")\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            try:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    word = random.choice(word_vectors.most_similar(positive=[word])[0:3])[0]\n",
    "                new_words.append(word)\n",
    "            except KeyError:\n",
    "                new_words.append(word)\n",
    "        random_text = ' '.join(new_words)\n",
    "        data_train.loc[data_train.index.max() + 1] = [0, 0, topic, random_text]\n",
    "    \n",
    "data_train = sklearn.utils.shuffle(data_train)\n",
    "print(data_train.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add generated data to train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data_train.append(data_valid).append(data_test)\n",
    "\n",
    "topic_count = {}\n",
    "for topic in topics:\n",
    "    topic_count[topic] = 0\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    topic_count[row['topic']] += 1\n",
    "    \n",
    "print(topic_count)\n",
    "\n",
    "print(data.shape, data_train.shape, data_valid.shape, data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either save or load generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# SAVE\n",
    "# with open(\"data_2.pickle\",\"wb\") as f:\n",
    "#     pickle.dump(data, f)\n",
    "# with open(\"data_train_2.pickle\",\"wb\") as f:\n",
    "#     pickle.dump(data_train, f)\n",
    "# with open(\"data_valid_2.pickle\",\"wb\") as f:\n",
    "#     pickle.dump(data_valid, f)\n",
    "# with open(\"data_test_2.pickle\",\"wb\") as f:\n",
    "#     pickle.dump(data_test, f)\n",
    "    \n",
    "# LOAD\n",
    "with open(\"data_2.pickle\",\"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "with open(\"data_train_2.pickle\",\"rb\") as f:\n",
    "    data_train = pickle.load(f)\n",
    "with open(\"data_valid_2.pickle\",\"rb\") as f:\n",
    "    data_valid = pickle.load(f)\n",
    "with open(\"data_test_2.pickle\",\"rb\") as f:\n",
    "    data_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data to feed into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "dic={}\n",
    "for i, topic in enumerate(topics):\n",
    "    dic[topic]=i\n",
    "train_labels=data_train.topic.apply(lambda x:dic[x])\n",
    "valid_labels=data_valid.topic.apply(lambda x:dic[x])\n",
    "test_labels=data_test.topic.apply(lambda x:dic[x])\n",
    "print(train_labels.shape, valid_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts=data.text\n",
    "print(texts.shape)\n",
    "\n",
    "NUM_WORDS = 20000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='\"#?$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences_train = tokenizer.texts_to_sequences(data_train.text)\n",
    "sequences_valid=tokenizer.texts_to_sequences(data_valid.text)\n",
    "sequences_test=tokenizer.texts_to_sequences(data_test.text)\n",
    "print(sequences_valid[0:2])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(sequences_train)\n",
    "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n",
    "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n",
    "y_train = to_categorical(np.asarray(train_labels[data_train.index]))\n",
    "y_val = to_categorical(np.asarray(valid_labels[data_valid.index]))\n",
    "y_test = to_categorical(np.asarray(test_labels[data_test.index]))\n",
    "print('Shape of train X and label tensor:', X_train.shape,y_train.shape)\n",
    "print('Shape of validation X and label tensor:', X_val.shape,y_val.shape)\n",
    "print('Shape of test X and label tensor:', X_test.shape,y_test.shape)\n",
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Some of the layers mad be deleted, not sure if they add any benefits. In the original paper their was no dropout after CNN layer, for activation a ReLU was used instead of LeakyReLU, and there was no batch norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding\n",
    "\n",
    "sequence_length = X_train.shape[1]\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "def create_model(lr = 0.01, num_filters = 30, drop1 = 0.1, drop = 0.5, kernel_regularizer = 0.01, alpha = 0.1, filter_sizes=[3,4,5,5]):    \n",
    "    K.clear_session()\n",
    "    inputs = Input(shape=(sequence_length,))\n",
    "    embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix.copy()],\n",
    "                            trainable=True)\n",
    "    embedding = embedding_layer(inputs)\n",
    "    reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "    all_maxpool = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        conv = Conv2D(num_filters, (filter_sizes[i], EMBEDDING_DIM),kernel_regularizer=regularizers.l2(kernel_regularizer))(reshape)\n",
    "        activation = LeakyReLU(alpha=alpha)(conv)\n",
    "        normal = BatchNormalization()(activation)\n",
    "        dropout = Dropout(drop1)(normal)\n",
    "        maxpool = MaxPooling2D((sequence_length - filter_sizes[i] + 1, 1), strides=(1,1))(dropout)\n",
    "        all_maxpool.append(maxpool)\n",
    "    merged_tensor = concatenate(all_maxpool, axis=1)\n",
    "    flatten = Flatten()(merged_tensor)\n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    output = Dense(units=len(topics), activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    adam = Adam(lr=lr)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different callbacks, most of the code is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath=\"checkpoints/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "filepath = \"checkpoints/weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# tensorboard = TensorBoard(log_dir='./Graph', histogram_freq=5, write_grads=True, batch_size=50, write_images=True, embeddings_freq=5)\n",
    "# tensorboard = TensorBoard(log_dir='./Graph', histogram_freq=5, write_grads=True, batch_size=50)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10),\n",
    "# callbacks = [early_stopping checkpoint, reduce_lr]\n",
    "callbacks = [early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform grid search for suitable hyperparams.\n",
    "\n",
    "Each new param: double the training time, be careful.\n",
    "\n",
    "Probably it's better to iterate over one param values and then go to the next one, it's faster but requires human supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    epochs=[100],\n",
    "    batch_size=[50, 500], # 554\n",
    "    lr=[0.001, 0.0001],\n",
    "    num_filters=[50, 100],\n",
    "    drop1=[0, 0.5],\n",
    "    drop=[0.5], \n",
    "    kernel_regularizer=[0.01, 3], \n",
    "    alpha=[0.1], \n",
    "    filter_sizes=[[2, 4, 5]]\n",
    ")\n",
    "# LOL\n",
    "results = []\n",
    "for epochs in param_grid['epochs']:\n",
    "    for batch_size in param_grid['batch_size']:\n",
    "        for lr in param_grid['lr']:\n",
    "            for num_filters in param_grid['num_filters']:\n",
    "                for drop1 in param_grid['drop1']:\n",
    "                    for drop in param_grid['drop']:\n",
    "                        for kernel_regularizer in param_grid['kernel_regularizer']:\n",
    "                            for alpha in param_grid['alpha']:\n",
    "                                for filter_sizes in param_grid['filter_sizes']:\n",
    "                                    model = create_model(\n",
    "                                        lr = lr, \n",
    "                                        num_filters = num_filters,\n",
    "                                        drop1 = drop1,\n",
    "                                        drop = drop,\n",
    "                                        kernel_regularizer = kernel_regularizer,\n",
    "                                        alpha = alpha, \n",
    "                                        filter_sizes=filter_sizes\n",
    "                                    )\n",
    "                                    current_params = {\n",
    "                                        'lr': lr, \n",
    "                                        'batch_size': batch_size,\n",
    "                                        'num_filters': num_filters,\n",
    "                                        'drop1': drop1,\n",
    "                                        'drop': drop,\n",
    "                                        'kernel_regularizer': kernel_regularizer,\n",
    "                                        'alpha': alpha, \n",
    "                                        'filter_sizes': filter_sizes\n",
    "                                    }\n",
    "                                    print(current_params)\n",
    "                                    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),\n",
    "                                             callbacks=callbacks)  # starts training\n",
    "                                    results.append({\n",
    "                                        'params': current_params,\n",
    "                                        'train': model.evaluate(X_train, y_train),\n",
    "                                        'valid': model.evaluate(X_val, y_val),\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort hyper params by the end result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.sort(key=lambda x: x['valid'][1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train without grid search and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = create_model(\n",
    "    lr = 0.0001, \n",
    "    num_filters = 20,\n",
    "    drop1 = 0.6,\n",
    "    drop = 0.6,\n",
    "    kernel_regularizer = 3,\n",
    "    alpha = 0.1, \n",
    "    filter_sizes=[2, 4, 5]\n",
    ")\n",
    "model.fit(X_train, y_train, batch_size=50, epochs=100, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to load model from saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_model(\n",
    "    lr = 0.0001, \n",
    "    num_filters = 20,\n",
    "    drop1 = 0.6,\n",
    "    drop = 0.6,\n",
    "    kernel_regularizer = 3,\n",
    "    alpha = 0.1, \n",
    "    filter_sizes=[2, 4, 5]\n",
    ")\n",
    "adam = Adam(lr=1e-3)\n",
    "model.load_weights(\"checkpoints/weights.best.hdf5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how trained model behaves on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequences_test=tokenizer.texts_to_sequences(data_test.text)\n",
    "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(scores[1] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See model in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pred_to_text(predictions, dic):\n",
    "    results = []\n",
    "    for prediction in predictions:\n",
    "        result_map = {}\n",
    "        for key in dic:\n",
    "            result_map[key] = prediction[dic[key]]\n",
    "        results.append(result_map)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_text = \"Please change the artwork\"\n",
    "sequences_test=tokenizer.texts_to_sequences([my_text])\n",
    "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n",
    "y_pred=model.predict(X_test)\n",
    "pred_to_text(y_pred, dic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "ts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
